= Usage

Trino works together with the Apache Hive metastore and S3 bucket.

== Prerequisites

* Deployed Stackable Apache Hive metastore
* Accessible S3 Bucket
    ** Endpoint, access-key and secret-key
    ** Data in the Bucket (we use the https://archive.ics.uci.edu/ml/datasets/iris[Iris] dataset here)
* Optional for authorization: Deployed Stackable OPA cluster + RegoRule server
* Optional to test queries with https://repo.stackable.tech/#browse/browse:packages:trino-cli%2Ftrino-cli-363-executable.jar[Trino CLI]

== Installation

In the following we explain or link the required installation steps.

=== S3 bucket

Please refer to the S3 provider.

=== Hive operator

Please refer to the https://github.com/stackabletech/hive-operator[Hive] operator and https://docs.stackable.tech/home/index.html[docs].

Both Hive and Trino need the same S3 authentication.

=== OPA operator

Please refer to the https://github.com/stackabletech/opa-operator[OPA] operator and https://docs.stackable.tech/home/index.html[docs].

=== Authentication

We provide user authentication via secret that can be referred in the custom resource:

  authentication:
    method: multiUser
    secrets:
      userCredentials:
        namespace: default
        name: simple-trino-users-secret

These secrets need to be created manually before startup (check `examples/simple-trino-users-secret.yaml`):

    kubectl apply -f examples/simple-trino-users-secret.yaml

The secret looks the following:

    apiVersion: v1
    kind: Secret
    metadata:
      name: simple-trino-users-secret
    type: kubernetes.io/opaque
    stringData:
      admin: $2y$10$89xReovvDLacVzRGpjOyAOONnayOgDAyIS2nW9bs5DJT98q17Dy5i
      alice: $2y$10$HcCa4k9v2DRrD/g7e5vEz.Bk.1xg00YTEHOZjPX7oK3KqMSt2xT8W
      bob: $2y$10$xVRXtYZnYuQu66SmruijPO8WHFM/UK5QPHTr.Nzf4JMcZSqt3W.2.

The <user>:<password> combinations are provided in the `stringData` field. Passwords need to be base64 encoded.

=== Regorule Server (Authorization)

The OPA cluster requires downloading rules from a RegoRule server. You can use your own solution or fall back on the Stackable Regorule Server

Please refer to the https://github.com/stackabletech/regorule-operator[RegoRule] operator and https://docs.stackable.tech/home/index.html[docs].

This is an example custom resource for the Stackable RegoRule server:

```
apiVersion: opa.stackable.tech/v1alpha1
kind: RegoRule
metadata:
  name: simple
spec:
  rego: |
    package trino

    can_execute_query = true

    can_access_catalog = true

    can_create_schema = true

    can_drop_schema = true

    can_access_schema = true

    can_create_table = true

    can_drop_table = true

    can_access_table = true

    can_access_column = true

    can_show_schemas = true

    can_show_tables = true

    default can_select_from_columns = false

    can_select_from_columns {
      input.request.table.catalog == "system"
      input.request.table.schema == "information_schema"
      input.request.table.table == {"tables", "schemata"}[_]
    }

    can_select_from_columns {
      input.request.table.catalog == "hive"
      input.request.table.schema == "iris"
      input.request.table.table == {"iris_parquet"}[_]
    }

    can_view_query_owned_by = true
```

You can let the Trino operator specify Rego rules by configuring the `authorization` field in the custom resource. This is a rudimentary implementation for user access.

=== Trino

With the prerequisites fulfilled, the CRD for this operator must be created:

    kubectl apply -f /etc/stackable/trino-operator/crd/trinocluster.crd.yaml

To create a single node Trino (v362) cluster with Prometheus metrics exposed on port 8081. Please adapt the `s3` with your credentials.

    cat <<EOF | kubectl apply -f -
    apiVersion: trino.stackable.tech/v1alpha1
    kind: TrinoCluster
    metadata:
      name: simple-trino
    spec:
      version: "0.0.362"
      nodeEnvironment: production
      hive:
        namespace: default
        name: simple-derby
      opa:
        namespace: default
        name: simple-opa
      authentication:
        method: multiUser
        secrets:
          userCredentials:
            namespace: default
            name: simple-trino-users-secret
      authorization:
        package: trino
        permissions:
          admin:
            schemas:
              read: true
              write: true
            tables:
              iris_parquet:
                read: true
                write: true
              iris_csv:
                read: true
                write: true
          bob:
            schemas:
              read: false
              write: false
            tables:
              iris_parquet:
                read: true
      s3:
        endPoint: changeme
        accessKey: changeme
        secretKey: changeme
        sslEnabled: false
        pathStyleAccess: true
      coordinators:
        roleGroups:
          default:
            selector:
              matchLabels:
                kubernetes.io/os: linux
            replicas: 1
            config:
              nodeDataDir: /stackable/trino/data
      workers:
        roleGroups:
          default:
            selector:
              matchLabels:
                kubernetes.io/os: linux
            replicas: 1
            config:
              nodeDataDir: /stackable/trino/data
    EOF

Assuming you've downloaded and installed the Trino client, connect to the Trino coordinator:

    ./trino.jar --debug --server https://<node_name>:<https-port> --user=admin --password

If you use self signed certificates, you also need to add `--insecure` to the command above.

Create a schema and a  table for the Iris data located in S3:

    CREATE SCHEMA IF NOT EXISTS hive.iris
    WITH (location = 's3a://iris/');

    CREATE TABLE IF NOT EXISTS hive.iris.iris_parquet (
      sepal_length DOUBLE,
      sepal_width  DOUBLE,
      petal_length DOUBLE,
      petal_width  DOUBLE,
      class        VARCHAR
    )
    WITH (
      external_location = 's3a://iris/parq',
      format = 'PARQUET'
    );

Query the data:

    SELECT
        sepal_length,
        class
    FROM hive.iris.iris_parquet
    LIMIT 10;

If you work with opa, try changing some RegoRule entries to false and see if you are not allowed to e.g. list tables or schemas.

When changing the automatically generated rego rule package name, a restart of the coordinator pod is required.