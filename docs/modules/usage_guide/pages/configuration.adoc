= Configuration

The cluster definition also supports overriding configuration properties and environment variables, either per role or per role group, where the more specific override (role group) has precedence over the less specific one (role).

IMPORTANT: Do not override port numbers. This will lead to faulty installations.

== Configuration Properties

For a role or role group, at the same level of `config`, you can specify: `configOverrides` for:

- `config.properties`
- `node.properties`
- `log.properties`
- `password-authenticator.properties`

For a list of possible configuration properties consult the https://trino.io/docs/current/admin/properties.html[Trino Properties Reference].

[source,yaml]
----
workers:
  roleGroups:
    default:
      config: {}
      replicas: 1
      configOverrides:
        config.properties:
          query.max-memory-per-node: "2GB"
----

Just as for the `config`, it is possible to specify this at role level as well:

[source,yaml]
----
workers:
  configOverrides:
    config.properties:
      query.max-memory-per-node: "2GB"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

All override property values must be strings. The properties will be passed on without any escaping or formatting.

== Environment Variables

Environment variables can be (over)written by adding the `envOverrides` property.

For example per role group:

[source,yaml]
----
workers:
  roleGroups:
    default:
      config: {}
      replicas: 1
      envOverrides:
        JAVA_HOME: "path/to/java"
----

or per role:

[source,yaml]
----
workers:
  envOverrides:
    JAVA_HOME: "path/to/java"
  roleGroups:
    default:
      config: {}
      replicas: 1
----

Here too, overriding properties such as `http-server.https.port` will lead to broken installations.

== Resources

=== Storage for data volumes

You can mount a volume where data (config and logs of Trino) is stored by specifying https://kubernetes.io/docs/concepts/storage/persistent-volumes[PersistentVolumeClaims] for each individual role or role group:

[source,yaml]
----
workers:
  config:
    resources:
      storage:
        data:
          capacity: 2Gi
  roleGroups:
    default:
      config:
        resources:
          storage:
            data:
              capacity: 3Gi
----

In the above example, all Trino workers in the default group will store data (the location of the property `--data-dir`) on a `3Gi` volume. Additional role groups not specifying any resources will inherit the config provided on the role level (`2Gi` volume). This works the same for memory or CPU requests.

By default, in case nothing is configured in the custom resource for a certain role group, each Pod will have a `2Gi` large local volume mount for the data location containing mainly logs.

=== Memory requests

You can request a certain amount of memory for each individual role group as shown below:

[source,yaml]
----
workers:
  roleGroups:
    default:
      config:
        resources:
          memory:
            limit: '2Gi'
----

In this example, each Trino container in the `default` group will have a maximum of 2 gigabytes of memory. To be more precise, these memory limits apply to the container running Trino but not to any sidecar containers that are part of the pod.

Setting this property will also automatically set the maximum Java heap size for the corresponding process to 80% of the available memory. Be aware that if the memory constraint is too low, the cluster might fail to start. If pods terminate with an 'OOMKilled' status and the cluster doesn't start, try increasing the memory limit.

For more details regarding Kubernetes memory requests and limits see: https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/[Assign Memory Resources to Containers and Pods].

=== CPU requests

Similarly to memory resources, you can also configure CPU limits, as shown below:

[source,yaml]
----
workers:
  roleGroups:
    default:
      config:
        resources:
          cpu:
            max: '500m'
            min: '250m'
----

=== Defaults

If nothing is specified, the operator will automatically set the following default values for resources:

[source,yaml]
----
workers:
  roleGroups:
    default:
      config:
        resources:
          requests:
            cpu: 200m
            memory: 2Gi
          limits:
            cpu: "4"
            memory: 2Gi
          storage:
            data:
              capacity: 2Gi
----

WARNING: The default values are _most likely_ not sufficient to run a proper cluster in production. Please adapt according to your requirements.

For more details regarding Kubernetes CPU limits see: https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/[Assign CPU Resources to Containers and Pods].